---
title: "WebScraping"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(rvest, dplyr, stringr)
```

![](https://rvest.tidyverse.org/logo.png){width=7%}

## Introdução a WebScraping

WebScraping  é a prática de coleta dados de endereços web de forma automática, usando um software ou script para coletar informações que seriam difíceis ou impossíveis de obter manualmente.

<!-- ## Estrutura WEB -->

<!-- ```{r image, echo=FALSE, fig.align='left',out.width = "10%"} -->
<!-- knitr::include_graphics("https://raw.githubusercontent.com/learnbr/html-css/master/logo.png") -->
<!-- ``` -->

<!-- HTML5 (Hypertext Markup Language), é uma das primeiras e principais linguagens de marcação da web e CSS3 (Cascading Style Sheets) define estilos e design do projeto web. -->

## Estrutura básica html

```
<!DOCTYPE html>
<html lang="pt-br">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">
    
    <link rel="stylesheet" href="style.css">
    
    <title>Posit</title>
</head>
<body>
    <nav class="navbar">
        <div class="pages">
            <a href="#">Linux</a>
            <a href="#">Windows</a>
            <a href="#">MacOS</a>
    </nav>

    <section class="content">
        <div class="content-ide">
            <img src="https://posit.co/wp-content/uploads/2022/09/posit.svg" alt="posit">

            <h1>RStudio IDE</h1>

            <p>O ambiente de codificação mais popular para R, 
                construído com amor por Posit. Usado por milhões 
                de pessoas semanalmente, o ambiente de desenvolvimento 
                integrado (IDE) RStudio é um conjunto de ferramentas 
                criadas para ajudá-lo a ser mais produtivo com R e Python. 
                Ele inclui um console e um editor de realce de sintaxe que 
                oferece suporte à execução direta de código. 
                Ele também possui ferramentas para plotagem, visualização 
                de histórico, depuração e gerenciamento de seu espaço de trabalho.</p>
        </div>
    </section>
</body>
</html>
```

`<table>` é um elemento usado para criar tabelas de dados. As tabelas são usadas para exibir informações organizadas em linhas e colunas, com cabeçalhos de coluna e linha opcionais. As tabelas são comumente usadas para exibir dados como horários, informações de produtos, resultados de pesquisa e muito mais.

```
<table>
  <thead>
    <tr>
      <th>Nome</th>
      <th>Sobrenome</th>
      <th>Idade</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>João</td>
      <td>Silva</td>
      <td>30</td>
    </tr>
    <tr>
      <td>Maria</td>
      <td>Santos</td>
      <td>25</td>
    </tr>
  </tbody>
</table>

```

`<thead>` define o cabeçalho da tabela ;
`<th>` define um cabeçalho de coluna;
`<tbody>` define o corpo da tabela;
`<tr>` define uma linha; 
`<td>` define as células da tabela.


<!-- ![](https://raw.githubusercontent.com/learnbr/html-css/master/logo.png) -->


## Criando uma `function` no R

Uma função é um bloco de código, definido por parâmetros que retorna dados como resultado.

```{r func_ex}

porcem <- function(valor, desconto){
  x <- (valor*desconto)/100
  print(paste0(desconto , "%" , " de ", valor, " é igual a ", x))
}

porcem(100,10)
porcem(323,18)
porcem(547,26)
porcem(1435,12)

```

## Criando função `get_table`

O primeiro passo para a criação da função de coleta de tabelas em `HTML5` é ter a url definidida e o caminho Xpath da tabela, extraído com auxílio do devTools do navegador. 

Xpath (XML Path) é uma linguagem utilizada exclusivamente para identificar ou endereçar as partes de um documento XML. Uma expressão XPath pode ser uada para procura em um documento XML e extrair informações de seus nós, que são qualquer parte de um documento, como um elemento ou atributo.

```{r get_table}

url <- "https://kworb.net/spotify/country/br_daily.html"

get_table <- function(url){
  url %>% 
    read_html(encoding="utf-8") %>% 
    html_nodes(xpath = '//*[@id="spotifydaily"]') %>% 
    html_table(header = TRUE)
}

```

A ideia do operador `pipe (%>%)` é realizar uma expressão em sequência.

Na função `get_table` o único paramêtro é o link da pagina web:

1. `read_html()` codifica a tabela em UTF-8 (8-bit Unicode Transformation Format é um tipo de codificação binária (Unicode) de comprimento variável)

2. `html_nodes()` identifica na url o caminho a ser coletado com o argumento ` xpath = ' ' `

3. `html_table()` identifica que a tabela há nome nas colunas com o parâmetro ` header = TRUE `
 
## Extraindo Tabela de um código HTML

```{r webscraping}
scraping <- sapply(url, get_table)

data <- do.call(rbind, scraping)
data <- as.data.frame(data)
```

A função `sapply()` usa lista, vetor ou quadro de dados como entrada e fornece saída em vetor ou matriz. É útil para operações em objetos de lista e retorna um objeto de lista do mesmo comprimento do conjunto original. A função Sapply em R faz o mesmo trabalho que a função lapply(), mas retorna um vetor.

A função `do.call()` constrói e executa uma ligação a partir de um nome ou função e uma lista de argumentos a serem passados para ela.


## WebScraping armazenado em `data.frame`

```{r result, echo=FALSE}
knitr::kable(head(data[c(1,3,4,7)],10))
```

## Coletando apenas elementos específicos
```{r specific}

link <- "https://www.rdocumentation.org/packages/rvest/versions/1.0.3"
link <- read_html(link)

```

### Por elemento e por rótulo
```{r}
# BUsca pelo elemento HTML
link %>% html_elements("h2") %>% html_text2()

# Busca pela class 
link %>% html_elements(".ml-2") %>% html_text2()
```

## WebScraping com Prudência com `Polite`

As duas funções principais do pacote, `bow` e `scrape`, definem e realizam uma sessão de coleta na web. 

`bow` é usado para apresentar o cliente ao host e pedir permissão para raspar (perguntando contra o arquivo `robots.txt` do host).

OBS: `O robots.txt é um arquivo salvo na pasta raiz do site com objetivo de guiar os robôs de busca.`

`scrape` é a função principal para recuperar dados do servidor remoto. Uma vez que a conexão é estabelecida, não há necessidade de bow novamente. 

```{r polite}
pacman::p_load(polite)

session <- bow("https://www.cheese.com/by_type", force = TRUE)
result <- scrape(session, query=list(t="semi-soft", per_page=100)) %>%
  html_node("#main-body") %>% 
  html_nodes("h3") %>% 
  html_text()

knitr::kable(head(result))
```

